{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUUnKJlBBoIDDFAtgUZ0wZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22046361/Coding--1-Homework-Submission/blob/main/saving_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e05RYPq7jek"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#Dont use scientific notation for numbers (e.g 1.003767687e-12)\n",
        "pd.set_option('display.float_format','{:.5f}'.format)\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collaborative Filtering\n",
        "\n",
        "\n",
        "\n",
        "### Importing Dataset"
      ],
      "metadata": {
        "id": "bzmsfjof7nRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('data/Preprocessed_data.csv')\n",
        "pd.options.display.max_columns = 150"
      ],
      "metadata": {
        "id": "kTXHVZ0h7lWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "uu3WZv6x7lZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "SPFGjQ5V7ldV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the size of the column\n",
        "column_size = len(data['user_id'])\n",
        "print(\"Size of user_id column:\", column_size)\n",
        "\n",
        "# Check the unique values in the column\n",
        "unique_values = data['user_id'].unique()\n",
        "print(\"Unique user IDs:\", unique_values)"
      ],
      "metadata": {
        "id": "-3UEqQRQ7lgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[(data['rating'] != 0) & (~data['rating'].isnull())]"
      ],
      "metadata": {
        "id": "mn4x5pSs7lkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I know by testing with the data, the dataset is too big for my computer to compute, so I will aim to reduce the size of the  dataset.  I first wanted to remove the ratings of 0, as I am not sure this would means that the use rated with a 0 or not rating the book. Therefore I will only keep a scale of rating 1 - >10.  I will then drop the rows of duplicates and the rows that have no book title."
      ],
      "metadata": {
        "id": "9tdeX0bu7wrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "uvAOCkLA7utS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate ratings by the same user for a book. Code from https://towardsdatascience.com/finding-and-removing-duplicate-rows-in-pandas-dataframe-c6117668631f#:~:text=To%20find%20duplicates%20on%20a,()%20method%20on%20the%20column.&text=The%20result%20is%20a%20boolean,identical%20to%20a%20previous%20one.\n",
        "duplicate_ratings = data[data.duplicated(subset=['user_id', 'book_title'], keep=False)]\n",
        "\n",
        "if len(duplicate_ratings) > 0:\n",
        "    print(\"Duplicate ratings found by the same user for the following books:\")\n",
        "    print(duplicate_ratings[['user_id', 'book_title']])\n",
        "else:\n",
        "    print(\"No duplicate ratings found.\")"
      ],
      "metadata": {
        "id": "LaNl6LK47u0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = data.drop_duplicates(subset=['user_id', 'book_title'], keep='first')"
      ],
      "metadata": {
        "id": "3R1uHEmz7u39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.shape"
      ],
      "metadata": {
        "id": "7g9nr6xl7u6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = new_data['book_title'].isnull().sum()\n",
        "zeros_count = (new_data['book_title'] == 0).sum()\n",
        "\n",
        "if missing_values > 0:\n",
        "    print(\"There are\", missing_values, \"missing values in the 'book_title' column.\")\n",
        "else:\n",
        "    print(\"There are no missing values in the 'book_title' column.\")\n",
        "\n",
        "if zeros_count > 0:\n",
        "    print(\"There are\", zeros_count, \"zeros in the 'book_title' column.\")\n",
        "else:\n",
        "    print(\"There are no zeros in the 'book_title' column.\")"
      ],
      "metadata": {
        "id": "3ED-7RKM7u9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BCgNOURT77cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the model, the book_titles together with the ratings will be the key parameters. As the book titles are non numerical I would need to chang this to help with the compute."
      ],
      "metadata": {
        "id": "1PrCjyRo79uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = new_data.copy()\n",
        "new_data.loc[:, 'book_id'] = pd.factorize(new_data['book_title'])[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assuming your DataFrame is named 'df' and the column with user IDs is 'user_id'\n",
        "new_data['user_id'] = pd.factorize(new_data['user_id'])[0]\n",
        "\n",
        "# Verify the updated values and size of the 'user_id' column\n",
        "print(\"Size of user_id column:\", len(new_data['user_id']))\n",
        "print(\"Unique user IDs:\", new_data['user_id'].unique())"
      ],
      "metadata": {
        "id": "AcdsiBi477fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.head"
      ],
      "metadata": {
        "id": "UZ43WTQg77ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=new_data.drop(['Unnamed: 0','location','isbn','img_s','img_m','img_l','Summary','city','state'],axis=1)"
      ],
      "metadata": {
        "id": "o3cj7z8K8C_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head"
      ],
      "metadata": {
        "id": "ySSYLR_n8DCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df[df['book_id'] == 0].index)"
      ],
      "metadata": {
        "id": "N4REd5mK8DGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head"
      ],
      "metadata": {
        "id": "PslFuu6H78FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the size of the column\n",
        "column_size = len(df['user_id'])\n",
        "print(\"Size of user_id column:\", column_size)\n",
        "\n",
        "# Check the unique values in the column\n",
        "unique_values = df['user_id'].unique()\n",
        "print(\"Unique user IDs:\", unique_values)"
      ],
      "metadata": {
        "id": "pUNkd4ww8KUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "abkY9h-h8KXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "R9UX-TAo8OVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "S7mnqwB_8OZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_subset = df['user_id'].unique()[:5000]\n",
        "df_subset = df[df['user_id'].isin(user_ids_subset)]"
      ],
      "metadata": {
        "id": "mYcuAn3A8R7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "user_ids = df_subset[\"user_id\"].unique().tolist()\n",
        "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
        "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
        "book_ids = df_subset[\"book_id\"].unique().tolist()\n",
        "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
        "book_encoded2book = {i: x for i, x in enumerate(book_ids)}\n",
        "\n",
        "# Check the size of user_ids list\n",
        "#num_unique_users = df_subset[\"user_id\"].nunique()\n",
        "#if len(user_ids) != num_unique_users:\n",
        "#    raise ValueError(\"The size of user_ids list does not match the number of unique user IDs in the dataset.\")\n",
        "\n",
        "#df_subset.loc[:, \"user\"] = df_subset[\"user_id\"].map(user2user_encoded)\n",
        "#df_subset.loc[:, \"book\"] = df_subset[\"book_id\"].map(book2book_encoded)\n",
        "\n",
        "df_subset[\"user\"] = df_subset[\"user_id\"].map(user2user_encoded)\n",
        "df_subset[\"book\"] = df_subset[\"book_id\"].map(book2book_encoded)\n",
        "\n",
        "\n",
        "\n",
        "num_users = len(user2user_encoded)\n",
        "num_books = len(book_encoded2book)\n",
        "#df_subset.loc[:, \"rating\"] = df_subset[\"rating\"].values.astype(np.float32)\n",
        "df_subset[\"rating\"] = df_subset[\"rating\"].values.astype(np.float32)\n",
        "min_rating = min(df_subset[\"rating\"])\n",
        "max_rating = max(df_subset[\"rating\"])\n",
        "\n",
        "print(\n",
        "    \"Number of users: {}, Number of books: {}, Min rating: {}, Max rating: {}\".format(\n",
        "        num_users, num_books, min_rating, max_rating\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "7mJ3p7Kt8R-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new code part 2\n",
        "\n",
        "## Prepare training and validation data\n",
        "\n",
        "df_subset = df_subset.sample(frac=1, random_state=42)\n",
        "x = df_subset[[\"user\", \"book\"]].values\n",
        "# Normalize the targets between 0 and 1. Makes it easy to train.\n",
        "y = df_subset[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n",
        "# Assuming training on 90% of the data and validating on 10%.\n",
        "train_indices = int(0.9 * df_subset.shape[0])\n",
        "x_train, x_val, y_train, y_val = (\n",
        "    x[:train_indices],\n",
        "    x[train_indices:],\n",
        "    y[:train_indices],\n",
        "    y[train_indices:],\n",
        ")"
      ],
      "metadata": {
        "id": "OyoJDpyw8SCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shapes of training data\n",
        "print(\"Training data shapes:\")\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the shapes of validation data\n",
        "print(\"\\nValidation data shapes:\")\n",
        "print(\"x_val shape:\", x_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)"
      ],
      "metadata": {
        "id": "0pRkLGkF8aWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = 50\n",
        "\n",
        "\n",
        "class RecommenderNet(keras.Model):\n",
        "    def __init__(self, num_users, num_books, embedding_size, **kwargs):\n",
        "        super(RecommenderNet, self).__init__(**kwargs)\n",
        "        self.num_users = num_users\n",
        "        self.num_books = num_books\n",
        "        self.embedding_size = embedding_size\n",
        "        self.user_embedding = layers.Embedding(\n",
        "            num_users,\n",
        "            embedding_size,\n",
        "            embeddings_initializer=\"he_normal\",\n",
        "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
        "        )\n",
        "        self.user_bias = layers.Embedding(num_users, 1)\n",
        "        self.book_embedding = layers.Embedding(\n",
        "            num_books,\n",
        "            embedding_size,\n",
        "            embeddings_initializer=\"he_normal\",\n",
        "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
        "        )\n",
        "        self.book_bias = layers.Embedding(num_books, 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_vector = self.user_embedding(inputs[:, 0])\n",
        "        user_bias = self.user_bias(inputs[:, 0])\n",
        "        book_vector = self.book_embedding(inputs[:, 1])\n",
        "        book_bias = self.book_bias(inputs[:, 1])\n",
        "        dot_user_book = tf.tensordot(user_vector, book_vector, 2)\n",
        "        # Add all the components (including bias)\n",
        "        x = dot_user_book + user_bias + book_bias\n",
        "        # The sigmoid activation forces the rating to between 0 and 1\n",
        "        return tf.nn.sigmoid(x)\n",
        "\n",
        "\n",
        "model = RecommenderNet(num_users, num_books, EMBEDDING_SIZE)\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(lr=0.001)\n",
        ")"
      ],
      "metadata": {
        "id": "kvjYpmzn8aZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "user_ids = df_subset[\"user_id\"].unique().tolist()\n",
        "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
        "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
        "book_ids = df_subset[\"book_id\"].unique().tolist()\n",
        "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
        "book_encoded2book = {i: x for i, x in enumerate(book_ids)}\n",
        "\n",
        "# Check the size of user_ids list\n",
        "num_unique_users = df_subset[\"user_id\"].nunique()\n",
        "if len(user_ids) != num_unique_users:\n",
        "    raise ValueError(\"The size of user_ids list does not match the number of unique user IDs in the dataset.\")\n",
        "\n",
        "df_subset.loc[:, \"user\"] = df_subset[\"user_id\"].map(user2user_encoded)\n",
        "df_subset.loc[:, \"book\"] = df_subset[\"book_id\"].map(book2book_encoded)\n",
        "\n",
        "num_users = len(user2user_encoded)\n",
        "num_books = len(book_encoded2book)\n",
        "df_subset.loc[:, \"rating\"] = df_subset[\"rating\"].values.astype(np.float32)\n",
        "min_rating = min(df_subset[\"rating\"])\n",
        "max_rating = max(df_subset[\"rating\"])\n",
        "\n",
        "print(\n",
        "    \"Number of users: {}, Number of books: {}, Min rating: {}, Max rating: {}\".format(\n",
        "        num_users, num_books, min_rating, max_rating\n",
        "    )\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "r4CZ3V2y8ac-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#part 3\n",
        "\n",
        "\n",
        "## Train the model based on the data split\n",
        "\n",
        "history = model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    verbose=1,\n",
        "    validation_data=(x_val, y_val),\n",
        ")\n",
        "\n",
        "\n",
        "## Plot training and validation loss\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o2pqkGMz8hD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9gLE6QY8hG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "probably not correct"
      ],
      "metadata": {
        "id": "eq7uIQp98mCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_movie(test_user_id, df, df_subset, book2book_encoded, user2user_encoded):\n",
        "    # Let us get a user and see the top recommendations.\n",
        "    books_read_by_user = df_subset[df_subset.user_id == test_user_id]\n",
        "    books_not_read = df[\n",
        "        ~df[\"book_id\"].isin(books_read_by_user.book_id.values)\n",
        "    ][\"book_id\"]\n",
        "    books_not_read = list(\n",
        "        set(books_not_read).intersection(set(book2book_encoded.keys()))\n",
        "    )\n",
        "    books_not_read = [[book2book_encoded.get(x)] for x in books_not_read]\n",
        "    user_encoder = user2user_encoded.get(test_user_id)\n",
        "    user_book_array = np.hstack(\n",
        "        ([[user_encoder]] * len(books_not_read), books_not_read)\n",
        "    )\n",
        "    ratings = model.predict(user_book_array).flatten()\n",
        "    top_ratings_indices = ratings.argsort()[-10:][::-1]\n",
        "    recommended_book_ids = [\n",
        "        book_encoded2book.get(books_not_read[x][0]) for x in top_ratings_indices\n",
        "    ]\n",
        "\n",
        "    print(\"Showing recommendations for user: {}\".format(test_user_id))\n",
        "    print(\"====\" * 9)\n",
        "    print(\"Books with high ratings from user\")\n",
        "    print(\"----\" * 8)\n",
        "    top_books_user = (\n",
        "        books_read_by_user.sort_values(by=\"rating\", ascending=False)\n",
        "        .head(5)\n",
        "        .book_id.values\n",
        "    )\n",
        "    df_rows = df[df[\"book_id\"].isin(top_books_user)]\n",
        "    for row in df_rows.itertuples():\n",
        "        print(row.title, \":\", row.genres)\n",
        "\n",
        "    print(\"----\" * 8)\n",
        "    print(\"Top 10 book recommendations\")\n",
        "    print(\"----\" * 8)\n",
        "    recommended_books = df[df[\"book_id\"].isin(recommended_book_ids)]\n",
        "    for row in recommended_books.itertuples():\n",
        "        print(row.title, \":\", row.genres)\n",
        "\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "JwuCJr_b8agF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yCKEeKCM8KbI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}